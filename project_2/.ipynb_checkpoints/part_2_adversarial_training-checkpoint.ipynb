{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import get_mnist_data\n",
    "from models import ConvNN\n",
    "from training_and_evaluation import train_model, predict_model\n",
    "from attacks import fast_gradient_attack\n",
    "from  torch.nn.functional import cross_entropy\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2, part 2: Adversarial training (20 pt)\n",
    "In this notebook we perform advcersarial training on the convolutional neural network from Part 1.\n",
    "\n",
    "## Your task\n",
    "Complete the missing code in this notebook. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n",
    "\n",
    "Specifically, for this part you will have to implement the following functions / classes:  \n",
    "\n",
    "**This notebook**\n",
    "* The `loss_function` used for adversarial training. (20pt)\n",
    "\n",
    "## General remarks\n",
    "\n",
    "Do not add or modify any code outside of the following comment blocks, or where otherwise explicitly stated.\n",
    "\n",
    "``` python\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Using additional `import` statements\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarise even for a single project task, you won't be eligible for the bonus this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'norm': '2', 'epsilon': 5}\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = get_mnist_data(train=True)\n",
    "mnist_testset = get_mnist_data(train=False)\n",
    "use_cuda = torch.cuda.is_available() #and False\n",
    "\n",
    "model = ConvNN()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 128\n",
    "test_batch_size = 1000  # feel free to change this\n",
    "lr = 1e-3\n",
    "\n",
    "opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "attack_args = {'norm': \"2\", \"epsilon\": 5}\n",
    "print(attack_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x: torch.Tensor, y: torch.Tensor, model: torch.nn.Module,  **attack_args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Loss function used for adversarial training. First computes adversarial examples on the input batch via fast_gradient_attack and then computes the logits\n",
    "    and the loss on the adversarial examples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor of shape [B, C, N, N], where B is the batch size, C is the number of channels, and N is the image width/height.\n",
    "        The input batch to certify.\n",
    "    y: torch.Tensor of shape [B, 1].\n",
    "        The labels of the input batch.\n",
    "    model: torch.nn.Module\n",
    "        The classifier to be evaluated.\n",
    "    attack_args: additional arguments passed to the adversarial attack function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple containing\n",
    "        * loss_pert: torch.Tensor, shape [B,]\n",
    "            The loss obtained on the adversarial examples.\n",
    "        * logits_pert: torch.Tensor, shape [B, K], where K is the number of classes.\n",
    "            The logits obtained on the adversarial examples\n",
    "    \"\"\"\n",
    "    ##########################################################\n",
    "    # YOUR CODE HERE\n",
    "    x = x.type(torch.cuda.FloatTensor)\n",
    "    x.requires_grad = True    \n",
    "    logits_original = model(x).cpu()\n",
    "    model.zero_grad()\n",
    "    x_perturbed = fast_gradient_attack(logits_original,x,y, attack_args[\"epsilon\"], attack_args[\"norm\"])   \n",
    "    \n",
    "    logits_pert = model(x_perturbed).cpu()\n",
    " \n",
    "\n",
    "    loss_pert = []\n",
    "    ele_choice = []\n",
    "    print(logits_pert)\n",
    "    log_logits = torch.log(logits_pert)\n",
    "    print(log_logits)\n",
    "    for i in range(x.shape[0]):\n",
    "        ele_choice, indices    = torch.topk(log_logits[i],2)\n",
    "        print(ele_choice)\n",
    "        if (indices[0] == y[i]):\n",
    "            max_ele = ele_choice[1]\n",
    "        else:\n",
    "            max_ele = ele_choice[0]\n",
    "            \n",
    "        print(i)\n",
    "        print(y[i])\n",
    "        loss_pert[i] = max((log_logits[i,y[i]] - max_ele),0)        \n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    # Important: don't forget to call model.zero_grad() after creating the adversarial examples.\n",
    "    return loss_pert, logits_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa9fe2d0c154bcc9ccbaa4669cd4138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=469), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3147,  1.3466,  1.0674,  ..., -0.4025,  1.5815,  0.8021],\n",
      "        [ 0.5118,  0.7748,  2.0112,  ..., -1.1114,  0.8852,  0.6282],\n",
      "        [ 0.1646,  0.7800,  1.4947,  ..., -0.5663,  0.1294,  0.3761],\n",
      "        ...,\n",
      "        [ 0.5467,  0.2524,  1.5707,  ..., -0.6201,  0.2462,  0.9306],\n",
      "        [-0.4160,  0.2024,  0.1225,  ...,  0.4234,  0.5036,  0.0375],\n",
      "        [ 0.8187,  0.4037,  1.6203,  ..., -0.4220, -0.3375,  1.0839]],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "tensor([nan, nan], grad_fn=<TopkBackward>)\n",
      "0\n",
      "tensor(5)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7ee8a039b14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_trainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/SS-20/Machine Learning for Graphs and Sequential Data/Assignments/project_2/training_and_evaluation.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, batch_size, loss_function, optimizer, epochs, loss_args)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mloss_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ef902438314b>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(x, y, model, **attack_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss_pert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_ele\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "losses, accuracies = train_model(model, mnist_trainset, batch_size=batch_size, loss_function=loss_function, optimizer=opt, loss_args = attack_args, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/adversarial_training.checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.subplot(122)\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_accuracy = predict_model(model, mnist_testset, batch_size=test_batch_size, attack_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_accuracy = predict_model(model, mnist_testset, batch_size=test_batch_size, attack_function=fast_gradient_attack, attack_args=attack_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
