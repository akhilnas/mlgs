{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from typing import List, Tuple\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: Graph Neural Networks (80 pt)\n",
    "\n",
    "Given the citation network `Cora ML` we are going to perform semi-supervised node classification (*transductive learning*), i.e. predict the category for each node given a few labels. Each node represents one publication and each edge a citation. The features represent the bag of words of the respective abstract after stemming and stop word removal. These papers are classified into one of the following seven classes:\n",
    "- Case Based\n",
    "- Genetic Algorithms\n",
    "- Neural Networks\n",
    "- Probabilistic Methods\n",
    "- Reinforcement Learning\n",
    "- Rule Learning\n",
    "- Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General remarks\n",
    "\n",
    "Do not add or modify any code outside of the following comment blocks, or where otherwise explicitly stated.\n",
    "\n",
    "``` python\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Below we list the allowed packages / no additional `import` statements\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarise even for a single project task, you won't be eligible for the bonus this semester.\n",
    "\n",
    "*For scalability reasons, please do not transform sparse matrices to dense (e.g. using `.to_dense()`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are allowed to use the following methods/packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import sparse as sp\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "For better reproducibility than in the last task - [*still, it might not be perfect*](https://pytorch.org/docs/stable/notes/randomness.html) :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if a GPU is available (or overwrite it with `cpu`)\n",
    "You are allowed to edit this line if it better fits you needs (only to change the value `use_cuda` - *this will affect the results*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "* `N` = number of publications (nodes in the graph)\n",
    "* `D` = number of features (bag of words one hot representation)\n",
    "<!--* `C` = number of categories-->\n",
    "* The graph is stored as a _sparse torch tensor_ `A` (shape `[N, N]`).\n",
    "* The (binary) Features are stored in a _feature tensor_ `X` (shape `[N, D]`).\n",
    "* The labels are stored in a _vector_ `y` (shape `[N]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.load('./X.pt')\n",
    "N, D = X.shape\n",
    "\n",
    "A_indices = torch.load('./A_indices.pt')\n",
    "A = torch.sparse.FloatTensor(A_indices, torch.ones_like(A_indices[0]).float(), (N, N)).coalesce()\n",
    "del A_indices\n",
    "\n",
    "labels = torch.load('./labels.pt')\n",
    "C = labels.max().item() + 1\n",
    "\n",
    "if use_cuda:\n",
    "    A, X, labels = A.cuda(), X.cuda(), labels.cuda()\n",
    "\n",
    "A, X, labels, N, D, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Graph Convolutional Network (35 pt) \n",
    "\n",
    "For the graph convolutional layer we are going to use the following update scheme:\n",
    "\n",
    "$$𝐻^{(𝑙+1)}=\\sigma\\left(𝐷^{−\\frac{1}{2}} 𝐴 𝐷^{−\\frac{1}{2}} 𝐻^{(𝑙)} 𝑊{(𝑙)}\\right)$$\n",
    "\n",
    "We use the ReLU for the activation function, but in the last layer where we directly output the raw logits (i.e. no activation at all). With $𝐻^{(0)}$ we denote the node features.\n",
    "\n",
    "\n",
    "### 1.1 - Implementation (25 pt)\n",
    "\n",
    "In this section your task is to implement a GCN in two steps. First we define the message passing / graph convolution module and then use this building block for a GCN.\n",
    "\n",
    "#### 1.1.1 - Graph Convolution Layer / Message Passing (5 pt)\n",
    "\n",
    "We also denote the normalized adjacency matrix as $\\hat{A} = 𝐷^{−\\frac{1}{2}} 𝐴 𝐷^{−\\frac{1}{2}}$. Here you are supposed to implement:\n",
    "\n",
    "$$Z^{(𝑙+1)}=\\hat{A} 𝐻^{(𝑙)} 𝑊{(𝑙)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolution Layer: as proposed in [Kipf et al. 2017](https://arxiv.org/abs/1609.02907).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels: int\n",
    "        Dimensionality of input channels/features.\n",
    "    out_channels: int\n",
    "        Dimensionality of output channels/features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "\n",
    "    def forward(self, arguments: Tuple[torch.tensor, torch.sparse.FloatTensor]) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        arguments: Tuple[torch.tensor, torch.sparse.FloatTensor]\n",
    "            Tuple of feature matrix `X` and normalized adjacency matrix `A_hat`\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        X: torch.tensor\n",
    "            The result of the message passing step\n",
    "        \"\"\"\n",
    "        X, A_hat = arguments\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        temp1 = sp.mm(X, A_hat)\n",
    "        X_new = self.linear(temp1)\n",
    "        X = X_new\n",
    "        ##########################################################\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 - Graph Convolution Network (20 pt)\n",
    "This task to two-fold: (1) you need to calculate `A_hat` in `_normalize(...)` and (2) connect the building blocks in `forward(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolution Network: as proposed in [Kipf et al. 2017](https://arxiv.org/abs/1609.02907).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features: int\n",
    "        Dimensionality of input features.\n",
    "    n_classes: int\n",
    "        Number of classes for the semi-supervised node classification.\n",
    "    hidden_dimensions: List[int]\n",
    "        Internal number of features. `len(hidden_dimensions)` defines the number of hidden representations.\n",
    "    activation: nn.Module\n",
    "        The activation for each layer but the last.\n",
    "    dropout: float\n",
    "        The dropout probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_features: int,\n",
    "                 n_classes: int,\n",
    "                 hidden_dimensions: List[int] = [80],\n",
    "                 activation: nn.Module = nn.ReLU(),\n",
    "                 dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self._layers = nn.ModuleList([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (f'gcn_{idx}', GraphConvolution(in_channels=in_channels,\n",
    "                                                out_channels=out_channels)),\n",
    "                (f'activation_{idx}', activation),\n",
    "                (f'dropout_{idx}', nn.Dropout(p=dropout))\n",
    "            ]))\n",
    "            for idx, (in_channels, out_channels)\n",
    "            in enumerate(zip([n_features] + hidden_dimensions[:-1], hidden_dimensions))\n",
    "        ] + [\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (f'gcn_{len(hidden_dimensions)}', GraphConvolution(in_channels=hidden_dimensions[-1],\n",
    "                                                    out_channels=n_classes))\n",
    "            ]))\n",
    "        ])\n",
    "        \n",
    "    def _normalize(self, A: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        For calculating $\\hat{A} = 𝐷^{−\\frac{1}{2}} 𝐴 𝐷^{−\\frac{1}{2}}$.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A: torch.sparse.FloatTensor\n",
    "            Sparse adjacency matrix with added self-loops.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A_hat: torch.sparse.FloatTensor\n",
    "            Normalized message passing matrix\n",
    "        \"\"\"\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        D = A.sum(-1).A1\n",
    "        D[D == 0] = 1\n",
    "        D = sp.diags(D ** (-1/2))\n",
    "        \n",
    "        A_hat = D @ A @ D\n",
    "        ##########################################################\n",
    "        return A_hat\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.tensor\n",
    "            Feature matrix `X`\n",
    "        A: torch.tensor\n",
    "            adjacency matrix `A` (with self-loops)\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        X: torch.tensor\n",
    "            The result of the last message passing step (i.e. the logits)\n",
    "        \"\"\"\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Normalise A Matrix\n",
    "        A_hat = self._normalize(A)\n",
    "        \n",
    "        # Forward Pass\n",
    "        X = self.layers((X,A_hat))\n",
    "        \n",
    "        ##########################################################\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_layer_gcn = GCN(n_features=D, n_classes=C, hidden_dimensions=[80, 80])\n",
    "if use_cuda:\n",
    "    three_layer_gcn = three_layer_gcn.cuda()\n",
    "    \n",
    "three_layer_gcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Training (10 pt)\n",
    "In the following we provide the `split` method for obtaining a train/validation/test-split. Subsequently, you will fill in the gap in the training loop:\n",
    "- Calculate the train and validation loss (we refer to slide 23 \"How to Perform Semi-Supervised Node Classification?\" for details).\n",
    "- You are given the `optimizer` and are supposed to perform the backward step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(labels: np.ndarray,\n",
    "          train_size: float = 0.025,\n",
    "          val_size: float = 0.025,\n",
    "          test_size: float = 0.95,\n",
    "          random_state: int = 42) -> List[np.ndarray]:\n",
    "    \"\"\"Split the arrays or matrices into random train, validation and test subsets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: np.ndarray [n_nodes]\n",
    "        The class labels\n",
    "    train_size: float\n",
    "        Proportion of the dataset included in the train split.\n",
    "    val_size: float\n",
    "        Proportion of the dataset included in the validation split.\n",
    "    test_size: float\n",
    "        Proportion of the dataset included in the test split.\n",
    "    random_state: int\n",
    "        Random_state is the seed used by the random number generator;\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    split_train: array-like\n",
    "        The indices of the training nodes\n",
    "    split_val: array-like\n",
    "        The indices of the validation nodes\n",
    "    split_test array-like\n",
    "        The indices of the test nodes\n",
    "\n",
    "    \"\"\"\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    idx_train_and_val, idx_test = train_test_split(idx,\n",
    "                                                   random_state=random_state,\n",
    "                                                   train_size=(train_size + val_size),\n",
    "                                                   test_size=test_size,\n",
    "                                                   stratify=labels)\n",
    "\n",
    "    idx_train, idx_val = train_test_split(idx_train_and_val,\n",
    "                                          random_state=random_state,\n",
    "                                          train_size=(train_size / (train_size + val_size)),\n",
    "                                          test_size=(val_size / (train_size + val_size)),\n",
    "                                          stratify=labels[idx_train_and_val])\n",
    "    \n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "idx_train, idx_val, idx_test = split(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "          X: torch.Tensor, \n",
    "          A: torch.sparse.FloatTensor, \n",
    "          labels: torch.Tensor, \n",
    "          idx_train: np.ndarray, \n",
    "          idx_val: np.ndarray,\n",
    "          lr: float = 1e-3,\n",
    "          weight_decay: float = 5e-4, \n",
    "          patience: int = 50, \n",
    "          max_epochs: int = 300, \n",
    "          display_step: int = 10):\n",
    "    \"\"\"\n",
    "    Train a model using either standard or adversarial training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model which we want to train.\n",
    "    X: torch.Tensor [n, d]\n",
    "        Dense attribute matrix.\n",
    "    A: torch.sparse.FloatTensor [n, n]\n",
    "        Sparse adjacency matrix.\n",
    "    labels: torch.Tensor [n]\n",
    "        Ground-truth labels of all nodes,\n",
    "    idx_train: np.ndarray [?]\n",
    "        Indices of the training nodes.\n",
    "    idx_val: np.ndarray [?]\n",
    "        Indices of the validation nodes.\n",
    "    lr: float\n",
    "        Learning rate.\n",
    "    weight_decay : float\n",
    "        Weight decay.\n",
    "    patience: int\n",
    "        The number of epochs to wait for the validation loss to improve before stopping early.\n",
    "    max_epochs: int\n",
    "        Maximum number of epochs for training.\n",
    "    display_step : int\n",
    "        How often to print information.\n",
    "    seed: int\n",
    "        Seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    trace_train: list\n",
    "        A list of values of the train loss during training.\n",
    "    trace_val: list\n",
    "        A list of values of the validation loss during training.\n",
    "    \"\"\"\n",
    "    trace_train = []\n",
    "    trace_val = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    for it in tqdm(range(max_epochs), desc='Training...'):\n",
    "        logits = model(X, A)     \n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Softmax Layer\n",
    "        softmax_layer = nn.Softmax(dim = 1)\n",
    "        p_v = softmax_layer(logits)\n",
    "        \n",
    "        # One-hot Vector Encoding \n",
    "        n_classes = np.unique(labels)\n",
    "        one_hot_encoding = np.eye(n_classes)[labels]\n",
    "        \n",
    "        # Loss\n",
    "        temp = np.multiply(one_hot_encoding,p_v)\n",
    "        temp = torch.from_numpy(temp)\n",
    "        loss_train = - temp.sum\n",
    "        loss_train.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Validation Dataset Loss\n",
    "        \n",
    "        \n",
    "        ##########################################################\n",
    "        \n",
    "        trace_train.append(loss_train.detach().item())\n",
    "        trace_val.append(loss_val.detach().item())\n",
    "\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "            best_epoch = it\n",
    "            best_state = {key: value.cpu() for key, value in model.state_dict().items()}\n",
    "        else:\n",
    "            if it >= best_epoch + patience:\n",
    "                break\n",
    "\n",
    "        if display_step > 0 and it % display_step == 0:\n",
    "            print(f'Epoch {it:4}: loss_train: {loss_train.item():.5f}, loss_val: {loss_val.item():.5f} ')\n",
    "\n",
    "    # restore the best validation state\n",
    "    model.load_state_dict(best_state)\n",
    "    return trace_train, trace_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_train, trace_val = train(three_layer_gcn, X, A, labels, idx_train, idx_val)\n",
    "\n",
    "plt.plot(trace_train, label='train')\n",
    "plt.plot(trace_val, label='validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Personalized Propagation of Neural Predictions (35 pt) \n",
    "\n",
    "We learned that a GCN comes with several limitations. Some of them are targeted via (Approximate) Personalized Propagation of Neural Predictions (A)PPNP.\n",
    "\n",
    "We use the iterative approach\n",
    "$$𝐻^{(𝑙+1)}= (1−\\alpha) \\delta_{\\text{dropout}} \\hat{A} 𝐻^{(𝑙)} + \\alpha 𝐻^{(0)}$$\n",
    "to approximate the personalized page rank. $\\alpha$ denotes the restart/teleport probability and $𝐻^{(0)}$ the result (i.e. logits) of a feed forward neural network on the input features $𝐻^{(0)}=f_{\\theta}(X)$. Looking at the details of [[Klicpera et al. 2019]](https://arxiv.org/abs/1810.05997) we understand that dropout is applied to the approximate propagation step (see upcoming Section 2.2). In each step, a randomly drawn dropout mask $\\delta_{\\text{dropout}}$ is multiplied with the normalized adjacency matrix $\\hat{A} = 𝐷^{−\\frac{1}{2}} 𝐴 𝐷^{−\\frac{1}{2}}$.\n",
    " \n",
    "### 2.1 - Sparse Dropout (10 pt)\n",
    "\n",
    "Unfortunately, PyTorch's dropout does not work on sparse matrices. However, we simply need to apply dropout to the values of the sparse matrix and then construct the sparse matrix again (`torch.sparse.FloatTensor(...)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dropout(A: torch.sparse.FloatTensor, p: float, training: bool) -> torch.sparse.FloatTensor:\n",
    "    ##########################################################\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    A=A.coalesce()# will remove NaN values. maybe not needed.\n",
    "    dropout = F.dropout(A._values(), p, training)\n",
    "    A=torch.sparse.FloatTensor(A._indices(), dropout, A.shape)\n",
    "    \n",
    "    ##########################################################\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Approximate Personalized Pagerank (15 pt)\n",
    "\n",
    "Implementation of the power iteration for approximating the personalized page rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerIterationPageRank(nn.Module):\n",
    "    \"\"\"\n",
    "    Power itertaion module for propagating the labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dropout: float\n",
    "        The dropout probability.\n",
    "    alpha: float\n",
    "        The teleport probability.\n",
    "    n_propagation: int\n",
    "        The number of iterations for approximating the personalized page rank.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dropout: float = 0.5,\n",
    "                 alpha: float = 0.15,\n",
    "                 n_propagation: int = 5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.n_propagation = n_propagation\n",
    "        \n",
    "    def forward(self, logits: torch.Tensor, A_hat: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        logits: torch.tensor\n",
    "            The local logits (for each node).\n",
    "        A_hat: torch.tensor\n",
    "            The normalized adjacency matrix `A_hat`.\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        logits: torch.tensor\n",
    "            The propagated/smoothed logits.\n",
    "        \"\"\"\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        A_hat=sparse_dropout(A_hat, self.dropout, training=True)\n",
    "        A_hat=(1-self.alpha)*A_hat\n",
    "        h_0=logits\n",
    "        for _ in range(self.n_propagation):\n",
    "            logits=A_hat@logits + self.alpha*h_0\n",
    "        #maybe should do softmax? or get just logit IDs. Shape of logits? \n",
    "        ##########################################################\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Approximate Personalized Propagation of Neural Predictions (10 pt)\n",
    "\n",
    "Putting it all together (note that you may use `self._normalize(...)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APPNP(GCN):\n",
    "    \"\"\"\n",
    "    Approximate Personalized Propagation of Neural Predictions: as proposed in [Klicpera et al. 2019](https://arxiv.org/abs/1810.05997).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features: int\n",
    "        Dimensionality of input features.\n",
    "    n_classes: int\n",
    "        Number of classes for the semi-supervised node classification.\n",
    "    hidden_dimensions: List[int]\n",
    "        Internal number of features. `len(hidden_dimensions)` defines the number of hidden representations.\n",
    "    activation: nn.Module\n",
    "        The activation for each layer but the last.\n",
    "    dropout: float\n",
    "        The dropout probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_features: int,\n",
    "                 n_classes: int,\n",
    "                 hidden_dimensions: List[int] = [80],\n",
    "                 activation: nn.Module = nn.ReLU(),\n",
    "                 dropout: float = 0.5,\n",
    "                 alpha: float = 0.1,\n",
    "                 n_propagation: int = 5):\n",
    "        super().__init__(n_features, n_classes)\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.dropout = dropout\n",
    "        self._transform_features = nn.Sequential(OrderedDict([\n",
    "            (f'dropout_{0}', nn.Dropout(p=self.dropout))\n",
    "        ] + list(chain(*[\n",
    "            [(f'linear_{idx}', nn.Linear(in_features=in_features, out_features=out_features)),\n",
    "             (f'activation_{idx}', activation)]\n",
    "            for idx, (in_features, out_features)\n",
    "            in enumerate(zip([n_features] + hidden_dimensions[:-1], hidden_dimensions))\n",
    "        ])) + [\n",
    "            (f'linear_{len(hidden_dimensions)}', nn.Linear(in_features=hidden_dimensions[-1],\n",
    "                                                        out_features=n_classes)),\n",
    "            (f'dropout_{len(hidden_dimensions)}', nn.Dropout(p=self.dropout)),\n",
    "        ]))\n",
    "        self._propagate = PowerIterationPageRank(dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 n_propagation=n_propagation)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.sparse.FloatTensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: torch.tensor\n",
    "            Feature matrix `X`\n",
    "        A: torch.tensor\n",
    "            adjacency matrix `A` (with self-loops)\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        logits: torch.tensor\n",
    "            The propagated logits.\n",
    "        \"\"\"\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        h_0=self._transform_features(X)\n",
    "        logits=self._propagate.forward(h_0, A)\n",
    "        ##########################################################\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPNP(\n",
       "  (_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (gcn_0): GraphConvolution(\n",
       "        (_linear): Linear(in_features=2879, out_features=80, bias=False)\n",
       "      )\n",
       "      (activation_0): ReLU()\n",
       "      (dropout_0): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (gcn_1): GraphConvolution(\n",
       "        (_linear): Linear(in_features=80, out_features=7, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_transform_features): Sequential(\n",
       "    (dropout_0): Dropout(p=0.5, inplace=False)\n",
       "    (linear_0): Linear(in_features=2879, out_features=80, bias=True)\n",
       "    (activation_0): ReLU()\n",
       "    (linear_1): Linear(in_features=80, out_features=80, bias=True)\n",
       "    (activation_1): ReLU()\n",
       "    (linear_2): Linear(in_features=80, out_features=7, bias=True)\n",
       "    (dropout_2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (_propagate): PowerIterationPageRank()\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_layer_appnp = APPNP(n_features=D, n_classes=C, hidden_dimensions=[80, 80])\n",
    "if use_cuda:\n",
    "    three_layer_appnp = three_layer_appnp.cuda()\n",
    "    \n",
    "three_layer_appnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66a1f1a7a2a4e2ea3ba50a51a5e720f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training...', max=300.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-52aec8fbf713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrace_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthree_layer_appnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d6a2c298a662>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, A, labels, idx_train, idx_val, lr, weight_decay, patience, max_epochs, display_step)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# One-hot Vector Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mone_hot_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mlgs/lib/python3.7/site-packages/numpy/lib/twodim_base.py\u001b[0m in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype, order)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "trace_train, trace_val = train(three_layer_appnp, X, A, labels, idx_train, idx_val)\n",
    "\n",
    "plt.plot(trace_train, label='train')\n",
    "plt.plot(trace_val, label='validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Oversmoothing (10 pt)\n",
    "As we have learned, we should limit the number of message passing steps for a vanilla GCN to prevent oversmoothing. In this section, we are going to analyze this phenomenon via plotting the test accuracy over the number of propagation steps.\n",
    "\n",
    "## 3.1 - Accuracy (5 pt)\n",
    "Please note that you are given the logits (no softmax applied), the labels, and the indices of the test nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(logits: torch.Tensor, labels: torch.Tensor, idx_test: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits: torch.tensor\n",
    "        The predicted logits.\n",
    "    labels: torch.tensor\n",
    "        The labels vector.\n",
    "    idx_test: torch.tensor\n",
    "        The indices of the test nodes.\n",
    "    \"\"\"\n",
    "    ##########################################################\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Select Logits coressponding to idx_test indices\n",
    "    logits_test = logits[idx_test,:]\n",
    "    \n",
    "    # Find Index of Maximum Logit\n",
    "    pred_labels = np.argmax(logits_test,axis=1)\n",
    "    \n",
    "    # Total Number of Elements\n",
    "    total_elements = len(idx_test)\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    matching = np.zeros(total_elements)\n",
    "    matching[pred_labels == labels] = 1\n",
    "    accuracy = np.sum(matching) / total_elements\n",
    "    \n",
    "    ##########################################################\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Compare GCN and APPNP (5 pt)\n",
    "\n",
    "Calculate the accuracy (keep in mind that dropout is only applied during training). Subsequently, we plot the accuracies over the numer of propagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_dimensions = 80\n",
    "n_propagations = [1,2,3,4,5,10]\n",
    "\n",
    "test_accuracy_gcn = []\n",
    "for n_propagation in n_propagations:\n",
    "    model = GCN(n_features=D, n_classes=C, hidden_dimensions=n_propagation*[n_hidden_dimensions])\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    train(model, X, A, labels, idx_train, idx_val, display_step=-1)\n",
    "    ##########################################################\n",
    "    # YOUR CODE HERE\n",
    "    logits = model(X,A)\n",
    "    accuracy = calc_accuracy(logits,labels,idx_test)\n",
    "    ##########################################################\n",
    "    test_accuracy_gcn.append(accuracy)\n",
    "    \n",
    "test_accuracy_appnp = []\n",
    "for n_propagation in n_propagations:\n",
    "    model = APPNP(n_features=D, n_classes=C, n_propagation=n_propagation)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    train(model, X, A, labels, idx_train, idx_val, display_step=-1)\n",
    "    ##########################################################\n",
    "    # YOUR CODE HERE\n",
    "    logits = model(X,A)\n",
    "    accuracy = calc_accuracy(logits,labels,idx_test)\n",
    "    ##########################################################\n",
    "    test_accuracy_appnp.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_propagations, test_accuracy_gcn, label='GCN', marker='.')\n",
    "plt.plot(n_propagations, test_accuracy_appnp, label='APPNP', marker='.')\n",
    "plt.xlabel('Message passing steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.7, 0.9)\n",
    "plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
